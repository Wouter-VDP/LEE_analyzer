{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEE Analyzer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes in the output directory of the jobs and convert it into a more flat pandas dataframe. \n",
    "Data from the root files will be partially processed to fields that are convenient to plot.\n",
    "The resulting dataframe will be pickled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from collections import OrderedDict\n",
    "\n",
    "from helpfunction import safely_reduce_dtype,reduce_mem_usage,sciNot,CheckBorderTPC,CheckBorderFixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "gr      = 1.618\n",
    "nan     = -2147483648\n",
    "min_root_size = 20000 # Skip root files smaller than x bytes\n",
    "\n",
    "mass_p= 0.93827 #GeV\n",
    "mass_e= 0.00511 #GeV\n",
    "\n",
    "# LAr EM showers\n",
    "R_moliere =  9.5 # cm\n",
    "X_o       = 13.9 # cm\n",
    "E_c       = 0.035# GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which plane do we want to use:\n",
    "plane = 2 # collection plane only\n",
    "\n",
    "# Fiducial volume borders in x,y,z:\n",
    "fid_arr= [[10,10],[20,20],[10,50]]\n",
    "# Fiducial volume for the end points of tracks\n",
    "fid_min = 10\n",
    "# Percentage cut for shower containment\n",
    "sh_cont_percent = .75\n",
    "\n",
    "# Minimum reconstructable energies:\n",
    "min_e = 0.02+mass_e # 20MeV\n",
    "min_p = 0.04+mass_p # 40MeV\n",
    "\n",
    "# Flat columns we want to copy from the original dataframe:\n",
    "flat_columns = [\"event\",\"subrun\",\"run\",\"nu_pdg\",\"nu_E\",\"true_vx_sce\",\"true_vy_sce\",\"true_vz_sce\",\n",
    "                \"distance\",\"category\",\"vx\",\"vy\",\"vz\",\"bnbweight\",\"passed\",\"candidate_pdg\",'numu_cuts','ccnc','qsqr','theta']\n",
    "\n",
    "vec_columns = [\"shower_open_angle\",\"shower_length\",\"shower_start_x\",\"shower_start_y\",\"shower_start_z\",\n",
    "              \"shower_dir_x\",\"shower_dir_y\",\"shower_dir_z\",\"shower_pca\",\n",
    "\n",
    "              \"track_start_x\",\"track_start_y\",\"track_start_z\",\"track_end_x\",\"track_end_y\",\"track_end_z\",\n",
    "              \"track_dir_x\",\"track_dir_y\",\"track_dir_z\",\"track_pca\",\n",
    "              \"predict_em\",\"predict_mu\",\"predict_cos\",\"predict_pi\",\"predict_p\",                                   # Katrin's BDT\n",
    "              \"track_pidchi\",\"track_pidchipr\",\"track_pidchika\",\"track_pidchipi\",\"track_pidchimu\",\"track_pida\",    # Adam's PID\n",
    "              \"track_res_mean\",\"track_res_std\", \n",
    "\n",
    "              \"true_shower_pdg\",\n",
    "              #\"true_shower_x_sce\",\"true_shower_y_sce\",\"true_shower_z_sce\",\"true_shower_depE\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/run/user/1000/gvfs/sftp:host=uboonegpvm02.fnal.gov,user=wvdp/uboone/data/users/wvdp/book/v06_26_01_12/cosmic_intime_1s0t/filesana.list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7aac43c9b86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgpvmsdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/run/user/1000/gvfs/sftp:host=uboonegpvm02.fnal.gov,user=wvdp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#filelist = [gpvmsdir+line.rstrip() for line in open(gpvmsdir+'/uboone/data/users/wvdp/book/v06_26_01_12/data_bnbext_b_1s0t/filesana.list')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgpvmsdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpvmsdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/uboone/data/users/wvdp/book/v06_26_01_12/cosmic_intime_1s0t/filesana.list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"valid ROOT files collected.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/run/user/1000/gvfs/sftp:host=uboonegpvm02.fnal.gov,user=wvdp/uboone/data/users/wvdp/book/v06_26_01_12/cosmic_intime_1s0t/filesana.list'"
     ]
    }
   ],
   "source": [
    "checkana=True\n",
    "\n",
    "if not checkana:\n",
    "    # Option 1: no checkana, verify validity yourself and use input directory\n",
    "    # Can be on gpvms, example: \"/run/user/1000/gvfs/sftp:host=uboonegpvm02.fnal.gov,user=wvdp/uboone/data/users/wvdp/v06_26_01_10/data_bnb_a_1e0p/*/*.root\"\n",
    "    # Local will be faster, avoid using pnfs scratch\n",
    "    inputlist = []\n",
    "    inputlist += glob.glob('/home/wouter/Templates/Run_March/bnb_nue_cosmic_1s0t/*/*.root')\n",
    "    #inputlist += glob.glob(gpvmsdir+'/uboone/data/users/wvdp/v06_26_01_12/data_bnb_b_1s0t/*/*.root')\n",
    "    filelist  = []\n",
    "    for fname in inputlist:\n",
    "        if (os.stat(fname).st_size)<min_root_size*80:\n",
    "                print(\"File\",fname,\"was corrupt. Size:\",os.stat(fname).st_size/1000, \"kb, skipping to next file.\")\n",
    "        else:\n",
    "            filelist.append(fname)\n",
    "\n",
    "if checkana:\n",
    "    # Option 2: After checkana, just pass the filesana list.\n",
    "    gpvmsdir = '/run/user/1000/gvfs/sftp:host=uboonegpvm02.fnal.gov,user=wvdp'\n",
    "    #filelist = [gpvmsdir+line.rstrip() for line in open(gpvmsdir+'/uboone/data/users/wvdp/book/v06_26_01_12/data_bnbext_b_1s0t/filesana.list')]\n",
    "    filelist = [gpvmsdir+line.rstrip() for line in open(gpvmsdir+'/uboone/data/users/wvdp/book/v06_26_01_12/cosmic_intime_1s0t/filesana.list')]\n",
    "\n",
    "print(len(filelist),\"valid ROOT files collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: List of lines that are causing warnings:\n",
    "\n",
    "#indexing-view-versus-copy\n",
    "#df_new[col]=dftemp[col].apply(safely_reduce_dtype)\n",
    "\n",
    "#VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
    "#RuntimeWarning: divide by zero encountered in float_scalars\n",
    "#RuntimeWarning: invalid value encountered in double_scalars\n",
    "#return pd.Series({\"shower_sp_profile\": np.mean(y[:l])/np.mean(y[-l:])})\n",
    "\n",
    "#RuntimeWarning: invalid value encountered in true_divide\n",
    "#center/=total_Q\n",
    "\n",
    "#RuntimeWarning: invalid value encountered in int_scalars\n",
    "#shower_hits_ratio[sh] = n_spacepoint[plane]/n_cluster[plane]\n",
    "#track_hits_ratio[tr] = n_spacepoint[plane]/n_cluster[plane]\n",
    "\n",
    "#RuntimeWarning: overflow encountered in double_scalars\n",
    "#dedx = [tr_E[0]*tr_cali[0],tr_E[1]*tr_cali[1],tr_E[2]*tr_cali[2],sum(nhits)/this_nhits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Signal Definition 1e0p \n",
    "def true_thresholds_1e0p(row):\n",
    "    if CheckBorderTPC(*row[[\"true_vx_sce\",\"true_vy_sce\",\"true_vz_sce\"]],array=fid_arr):\n",
    "        passed_e=False\n",
    "        for pdg,E in zip(*row[[\"nu_daughters_pdg\",\"nu_daughters_E\"]]):\n",
    "            if pdg==11 and E>min_e:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def KatrinBDT(row):\n",
    "    if np.all(row[\"predict_p\"]>0.001):\n",
    "        if np.all(row[\"predict_mu\"]<0.6):\n",
    "            if np.all(row[\"predict_cos\"]<0.6):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verifies if the event passed the flash precuts.\n",
    "def FlashPrecut(row):\n",
    "    flash_ok = False\n",
    "    t_start = 3.2\n",
    "    t_end   = t_start+1.6\n",
    "    min_PE  = 50\n",
    "\n",
    "    for time,PE in zip(*row[[\"flash_time\",\"flash_PE\"]]):\n",
    "        if time>t_start and time<t_end and PE>min_PE:\n",
    "            flash_ok = True\n",
    "    return pd.Series({\"flash_precut\": flash_ok}) \n",
    "\n",
    "\n",
    "\n",
    "# Reconstructed energy using collection plane\n",
    "def CalcRecoE(row):\n",
    "    reco_energy = 0\n",
    "    total_nhits = 0 # sum over all the planes\n",
    "    shower_nhits = np.zeros(row[\"n_showers\"]) \n",
    "    shower_energy = np.zeros(row[\"n_showers\"])\n",
    "    shower_cali = np.zeros(row[\"n_showers\"])\n",
    "    shower_hits_ratio = np.zeros(row[\"n_showers\"])\n",
    "    \n",
    "    track_nhits = np.zeros(row[\"n_tracks\"])\n",
    "    track_energy = np.zeros(row[\"n_tracks\"])\n",
    "    track_cali = np.zeros(row[\"n_tracks\"])\n",
    "    track_hits_ratio = np.zeros(row[\"n_tracks\"])\n",
    "    \n",
    "    for tr, tr_E, n_cluster, n_spacepoint, cali in zip(range(row[\"n_tracks\"]),*row[[\"track_energy_hits\",\"track_nhits_cluster\",\"track_nhits_spacepoint\",\"track_energy_cali\"]] ):\n",
    "        reco_energy+=tr_E[plane]\n",
    "        track_nhits[tr] = int(n_cluster[plane])\n",
    "        track_energy[tr] = tr_E[plane]\n",
    "        track_cali[tr] = cali[plane]\n",
    "        track_hits_ratio[tr] = n_spacepoint[plane]/n_cluster[plane]\n",
    "        total_nhits+=int(sum(n_cluster))\n",
    "        \n",
    "    for sh, sh_E, n_cluster, n_spacepoint, cali in zip(range(row[\"n_showers\"]),*row[[\"shower_energy_hits\",\"shower_nhits_cluster\",\"shower_nhits_spacepoint\",\"shower_energy_cali\"]] ):\n",
    "        reco_energy+=sh_E[plane]\n",
    "        shower_nhits[sh] = int(n_cluster[plane])\n",
    "        shower_energy[sh] = sh_E[plane]\n",
    "        shower_cali[sh] = cali[plane]\n",
    "        shower_hits_ratio[sh] = n_spacepoint[plane]/n_cluster[plane]\n",
    "        total_nhits+=int(sum(n_cluster))\n",
    "        \n",
    "    return pd.Series({\"reconstructed_energy\": reco_energy, \"total_nhits\": total_nhits,\n",
    "                      \"shower_nhits\": shower_nhits, \"shower_energy\": shower_energy, \"shower_cali\": shower_cali, \"shower_hits_ratio\" : shower_hits_ratio,\n",
    "                      \"track_nhits\": track_nhits, \"track_energy\": track_energy, \"track_cali\": track_cali, \"track_hits_ratio\" : track_hits_ratio})\n",
    "\n",
    "\n",
    "# Add for every oject the angle it has with it's most opposite object:\n",
    "def MaxAngle(row):\n",
    "    track_maxangle = np.zeros(row[\"n_tracks\"])\n",
    "    shower_maxangle = np.zeros(row[\"n_showers\"])\n",
    "    \n",
    "    track_dirs = zip(*row[['track_dir_x','track_dir_y','track_dir_z']])\n",
    "    shower_dirs = zip(*row[['shower_dir_x','shower_dir_y','shower_dir_z']])\n",
    "    alldir = list(track_dirs) + list(shower_dirs)\n",
    "    #print(track_dirs)\n",
    "    #print(alldir)\n",
    "    \n",
    "    for nr,(this_dir_x, this_dir_y, this_dir_z) in enumerate(zip(*row[['track_dir_x','track_dir_y','track_dir_z']])):\n",
    "        cosine=1\n",
    "        for dir_x, dir_y, dir_z in alldir:\n",
    "            u=np.array([this_dir_x,this_dir_y,this_dir_z])\n",
    "            v=np.array([dir_x,dir_y,dir_z])\n",
    "            if np.array_equal(u,v):\n",
    "                continue\n",
    "            this_cosine = np.dot(u,v)/np.linalg.norm(u)/np.linalg.norm(v)\n",
    "            if this_cosine < cosine:\n",
    "                cosine=this_cosine\n",
    "        track_maxangle[nr]=cosine\n",
    "        \n",
    "    for nr,(this_dir_x, this_dir_y, this_dir_z) in enumerate(zip(*row[['shower_dir_x','shower_dir_y','shower_dir_z']])):\n",
    "        cosine=1\n",
    "        for dir_x, dir_y, dir_z in alldir:\n",
    "            u=np.array([this_dir_x,this_dir_y,this_dir_z])\n",
    "            v=np.array([dir_x,dir_y,dir_z])\n",
    "            if np.array_equal(u,v):\n",
    "                continue\n",
    "            this_cosine = np.dot(u,v)/np.linalg.norm(u)/np.linalg.norm(v)\n",
    "            if this_cosine < cosine:\n",
    "                cosine=this_cosine\n",
    "        shower_maxangle[nr]=cosine\n",
    "    \n",
    "    return pd.Series({\"track_maxangle\": track_maxangle, \"shower_maxangle\": shower_maxangle })\n",
    "                   \n",
    "    \n",
    "\n",
    "# Add info about the hierarchy, 0= no daughter, 1=has showerdaughter, 2=has trackdaughter, 3=more than 1\n",
    "def DaughterInfo(row):\n",
    "    nu_shower_ids, nu_track_ids                = row[\"nu_shower_ids\"],row[\"nu_track_ids\"]\n",
    "    nu_shower_daughters, nu_track_daughters    = list(row[\"nu_shower_daughters\"]),list(row[\"nu_track_daughters\"])\n",
    "\n",
    "    showerdaughter = []\n",
    "    for sh in nu_shower_daughters:\n",
    "        if len(sh)==0:\n",
    "            showerdaughter.append(0)\n",
    "        elif len(sh)==1:\n",
    "            if sh[0] in nu_shower_ids:\n",
    "                showerdaughter.append(1)\n",
    "            elif sh[0] in nu_track_ids:\n",
    "                showerdaughter.append(2)\n",
    "        else:\n",
    "            showerdaughter.append(3)\n",
    "            #print(\"Shower had more than one daughter\")\n",
    "            \n",
    "    trackdaughter = []\n",
    "    for tr in nu_track_daughters:\n",
    "        if len(tr)==0:\n",
    "            trackdaughter.append(0)\n",
    "        elif len(tr)==1:\n",
    "            if tr[0] in nu_shower_ids:\n",
    "                trackdaughter.append(1)\n",
    "            elif tr[0] in nu_track_ids:\n",
    "                trackdaughter.append(2)\n",
    "        else:\n",
    "            trackdaughter.append(3)\n",
    "            #print(\"Track had more than one daughter\")\n",
    "            \n",
    "    \n",
    "    nu_shower_daughters     = np.hstack(row[\"nu_shower_daughters\"]) # want them as flat numpy arrays\n",
    "    if len(nu_track_daughters)>0:\n",
    "        nu_track_daughters= np.hstack(row[\"nu_track_daughters\"])\n",
    "    \n",
    "    if len(nu_shower_daughters)+len(nu_track_daughters) ==0:\n",
    "        shower_is_daughter=[0]*len(nu_shower_ids)\n",
    "        track_is_daughter=[0]*len(nu_track_ids)\n",
    "    else:\n",
    "        shower_is_daughter = []\n",
    "        for sh in nu_shower_ids:\n",
    "            if sh in nu_shower_daughters:\n",
    "                shower_is_daughter.append(1)\n",
    "            elif sh in nu_track_daughters:\n",
    "                shower_is_daughter.append(2)\n",
    "            else:\n",
    "                shower_is_daughter.append(0)\n",
    "\n",
    "        track_is_daughter = []\n",
    "        for tr in nu_track_ids:\n",
    "            if tr in nu_shower_daughters:\n",
    "                track_is_daughter.append(1)\n",
    "            elif tr in nu_track_daughters:\n",
    "                track_is_daughter.append(2)\n",
    "            else:\n",
    "                track_is_daughter.append(0)\n",
    "            \n",
    "    return pd.Series({\"shower_daughter\": showerdaughter, \"track_daughter\": trackdaughter,\n",
    "                      \"shower_is_daughter\": shower_is_daughter, \"track_is_daughter\": track_is_daughter,\n",
    "                     })     \n",
    "    \n",
    "\n",
    "# Calculates the true end point for electron showers, for 95% of energy\n",
    "def ShowerTrueEnd(row):\n",
    "    if 11 not in row[\"nu_daughters_pdg\"]:\n",
    "        return pd.Series({\"true_shower_endx\": -999.0, \"true_shower_endy\": -999.0, \"true_shower_endz\": -999.0,\n",
    "                      \"true_shower_tmax_x\": -999.0, \"true_shower_tmax_y\": -999.0, \"true_shower_tmax_z\": -999.0})\n",
    "    i_daughter = np.nonzero(row[\"nu_daughters_pdg\"]==11)\n",
    "    if len(i_daughter[0])>1:\n",
    "        print(\"More than 1 true electron daughter\")\n",
    "    i_daughter = i_daughter[0][0]\n",
    "    \n",
    "    E_ratio = (row[\"nu_daughters_E\"][i_daughter])/E_c\n",
    "    t_max = np.log(E_ratio)-1.0\n",
    "    length = (t_max+0.08*18+9.6)*X_o\n",
    "    #print(\"E_ratio\",E_ratio,\"E\",row[\"nu_daughters_E\"][i_daughter],\" t_max\",t_max,\"length\",length)\n",
    "    direction = np.array([row[\"nu_daughters_px\"][i_daughter],row[\"nu_daughters_py\"][i_daughter],row[\"nu_daughters_pz\"][i_daughter]])\n",
    "    true_shower_start = np.array([row[\"nu_daughters_vx\"][i_daughter],row[\"nu_daughters_vy\"][i_daughter],row[\"nu_daughters_vz\"][i_daughter]])\n",
    "    true_shower_end = true_shower_start+length*direction/np.linalg.norm(direction)\n",
    "    true_shower_tmax = true_shower_start+(t_max*X_o)*direction/np.linalg.norm(direction)\n",
    "    \n",
    "    return pd.Series({\"true_shower_endx\": true_shower_end[0], \"true_shower_endy\": true_shower_end[1], \"true_shower_endz\": true_shower_end[2],\n",
    "                      \"true_shower_tmax_x\": true_shower_tmax[0], \"true_shower_tmax_y\": true_shower_tmax[1], \"true_shower_tmax_z\": true_shower_tmax[2]})\n",
    "\n",
    "\n",
    "def TrueDaughterInfo(row):\n",
    "    #\"nu_daughters_pdg\",\"nu_daughters_E\",\n",
    "    #\"nu_daughters_px\",\"nu_daughters_py\",\"nu_daughters_pz\",\n",
    "    #\"nu_daughters_endx\",\"nu_daughters_endy\",\"nu_daughters_endz\"\n",
    "    nu_daughters_pdg = []\n",
    "    nu_daughters_E = []\n",
    "    for pdg,E in zip(*row[[\"nu_daughters_pdg\", \"nu_daughters_E\"]]):\n",
    "        if pdg!=2112 and pdg<9999: #Do not store neutrons and nuclei\n",
    "            nu_daughters_pdg.append(pdg)\n",
    "            nu_daughters_E.append(E)\n",
    "    return pd.Series({\"nu_daughters_pdg\": np.array(nu_daughters_pdg), \"nu_daughters_E\": np.array(nu_daughters_E)})\n",
    "\n",
    "    \n",
    "\n",
    "# Calculates the percentage of sps inside the fiducial volume and the percentage of deposited collection plane charge\n",
    "def ContainedRatio(row):\n",
    "    d=0.01\n",
    "    n=0.0\n",
    "    \n",
    "    for x,y,z,q in zip(*row[[\"shower_sp_x\", \"shower_sp_y\", \"shower_sp_z\", \"shower_sp_int\"]]):\n",
    "        d+=q\n",
    "        if CheckBorderTPC(x,y,z,array=fid_arr):\n",
    "            n+=q\n",
    "\n",
    "    return pd.Series({\"shower_containment_q\": n/d}) \n",
    "\n",
    "\n",
    "\n",
    "def CC_daughter_E(row):\n",
    "    CC_daughter_i = np.nonzero(np.in1d(row[\"nu_daughters_pdg\"], [11,-11,13,-13]))[0]\n",
    "    CC_daughter_E = -1\n",
    "    if len(CC_daughter_i)>0:\n",
    "        if len(CC_daughter_i)>2:\n",
    "            \"Multiple electron/muon daughters found!\"\n",
    "        else:      \n",
    "            CC_daughter_E = row[\"nu_daughters_E\"][CC_daughter_i[0]]\n",
    "    return pd.Series({\"CC_daughter_E\": CC_daughter_E})  \n",
    "\n",
    "\n",
    "\n",
    "# Returns the ratio of collection charge of the first part and the second part of the summed shower.\n",
    "def ShowerChargeProfile(row):\n",
    "    x,y,z = row[\"vx\"],row[\"vy\"],row[\"vz\"]\n",
    "    \n",
    "    center= np.array([0.0,0.0,0.0])\n",
    "    total_Q = 0.0\n",
    "    for sps_x,sps_y,sps_z,sps_int in zip(*row[[\"shower_sp_x\",\"shower_sp_y\",\"shower_sp_z\",\"shower_sp_int\"]]):\n",
    "        center+=np.array([sps_x,sps_y,sps_z])*sps_int\n",
    "        total_Q+=sps_int\n",
    "    center/=total_Q\n",
    "    norm = (center-np.array([x,y,z])) / np.linalg.norm(center-np.array([x,y,z]))\n",
    "    \n",
    "    distance = []\n",
    "    weights = []\n",
    "    for sps_x,sps_y,sps_z in zip(*row[[\"shower_sp_x\",\"shower_sp_y\",\"shower_sp_z\"]]):\n",
    "        if sps_int>0:\n",
    "            distance.append( np.dot([sps_x-x,sps_y-y,sps_z-z],norm) )\n",
    "            weights.append(sps_int)\n",
    "            \n",
    "    y,x = np.histogram( distance, weights = weights )\n",
    "    l = len(y)/2\n",
    "    \n",
    "    #ratio = np.mean(y[:l])/np.mean(y[-l:]) if np.mean(y[-l:])>0 else -1\n",
    "    return pd.Series({\"shower_sp_profile\": np.mean(y[:l])/np.mean(y[-l:])})  \n",
    "\n",
    "\n",
    "# Returns the dedx and the number of hits it had to compute this.\n",
    "def CalcDedx(row):\n",
    "    shower_dedx_hits = np.zeros(row[\"n_showers\"]) \n",
    "    shower_dedx      = np.zeros(row[\"n_showers\"])\n",
    "    shower_dedx_avg  = np.zeros(row[\"n_showers\"]) \n",
    "    shower_dedx_cali = np.zeros(row[\"n_showers\"]) \n",
    "    \n",
    "    track_dedx_hits  = np.zeros(row[\"n_tracks\"])\n",
    "    track_dedx       = np.zeros(row[\"n_tracks\"])\n",
    "    track_dedx_avg   = np.zeros(row[\"n_tracks\"])\n",
    "    track_dedx_cali  = np.zeros(row[\"n_tracks\"]) \n",
    "    \n",
    "    \n",
    "    for tr, tr_E, nhits,tr_cali in zip(range(row[\"n_tracks\"]),*row[[\"track_dEdx\",\"track_dEdx_hits\",\"track_dQdx_cali\"]]):\n",
    "        this_nhits = len(nhits)\n",
    "        track_dedx_hits[tr] = this_nhits\n",
    "        if this_nhits==0:\n",
    "            track_dedx_avg[tr]  = 0\n",
    "            track_dedx[tr]      = 0\n",
    "            track_dedx_cali[tr] = 0\n",
    "        else:\n",
    "            dedx = [tr_E[0]*tr_cali[0],tr_E[1]*tr_cali[1],tr_E[2]*tr_cali[2],sum(nhits)/this_nhits]\n",
    "            #just force the code to take the best dedx if it would be an electron.\n",
    "            track_dedx_avg[tr]  = dedx[np.argmin(abs(np.array(dedx)-1.95))]\n",
    "            track_dedx[tr]      = tr_E[plane]\n",
    "            track_dedx_cali[tr] = tr_cali[plane]\n",
    "        \n",
    "    for sh, sh_E, nhits,sh_cali in zip(range(row[\"n_showers\"]),*row[[\"shower_dEdx\",\"shower_dEdx_hits\",\"shower_dQdx_cali\"]]):\n",
    "        this_nhits = len(nhits)\n",
    "        shower_dedx_hits[sh] = this_nhits\n",
    "        if this_nhits==0:\n",
    "            shower_dedx_avg[sh]  = 0\n",
    "            shower_dedx[sh]      = 0\n",
    "            shower_dedx_cali[sh] = 0\n",
    "        else:\n",
    "            dedx = [sh_E[0]*sh_cali[0],sh_E[1]*sh_cali[1],sh_E[2]*sh_cali[2],sum(nhits)/this_nhits]\n",
    "            #just force the code to take the best dedx if it would be an electron.\n",
    "            shower_dedx_avg[sh]  = dedx[np.argmin(abs(np.array(dedx)-1.95))]\n",
    "            shower_dedx[sh]      = sh_E[plane]\n",
    "            shower_dedx_cali[sh] = sh_cali[plane]\n",
    "    \n",
    "    return pd.Series({\"shower_dedx_hits\": shower_dedx_hits, \"shower_dedx\": shower_dedx, \"shower_dedx_avg\": shower_dedx_avg, \"shower_dedx_cali\" : shower_dedx_cali,\n",
    "                      \"track_dedx_hits\": track_dedx_hits, \"track_dedx\": track_dedx, \"track_dedx_avg\": track_dedx_avg, \"track_dedx_cali\" : track_dedx_cali})\n",
    "\n",
    "def OpticalInfo(row):\n",
    "    flash_PE=0\n",
    "    flash_time=0\n",
    "    if np.any(row[\"flash_passed\"]!=-1):\n",
    "        flash_PE = row[\"flash_PE\"][np.max(row[\"flash_passed\"])]\n",
    "        flash_time = row[\"flash_time\"][np.max(row[\"flash_passed\"])]\n",
    "    return pd.Series({\"flash_time\": flash_time, \"flash_PE\": flash_PE})\n",
    "\n",
    "\n",
    "def MatchedCleanup(row):    \n",
    "    nu_shower_ids, nu_track_ids                = row[\"nu_shower_ids\"],row[\"nu_track_ids\"]\n",
    "    nu_shower_daughters, nu_track_daughters    = row[\"nu_shower_daughters\"],row[\"nu_track_daughters\"]\n",
    "    \n",
    "    matched_tracks= row[\"matched_tracks\"]\n",
    "    matched_tracks_energy= row[\"matched_tracks_energy\"]\n",
    "    \n",
    "    matched_showers= row[\"matched_showers\"]\n",
    "    matched_showers_energy= row[\"matched_showers_energy\"]\n",
    "\n",
    "    if len(np.hstack(nu_shower_daughters))>0:\n",
    "        for daughters,pdg,energy in zip(nu_shower_daughters,matched_showers,matched_showers_energy):\n",
    "            if len(daughters)>0:\n",
    "                for daughter in daughters:\n",
    "                    if daughter in nu_shower_ids:\n",
    "                        index = np.where(nu_shower_ids==daughter)\n",
    "                        matched_showers[index[0]]=pdg\n",
    "                        matched_showers_energy[index[0]] = energy\n",
    "                    if daughter in nu_track_ids:\n",
    "                        index = np.where(nu_track_ids==daughter)\n",
    "                        matched_tracks[index[0]]=pdg\n",
    "                        matched_tracks_energy[index[0]] = energy\n",
    "                        \n",
    "    if len(nu_track_daughters)>0:\n",
    "        if len(np.hstack(nu_track_daughters))>0:\n",
    "            for daughters,pdg,energy in zip(nu_track_daughters,matched_tracks,matched_tracks_energy):\n",
    "                if len(daughters)>0:\n",
    "                    for daughter in daughters:\n",
    "                        if daughter in nu_shower_ids:\n",
    "                            index = np.where(nu_shower_ids==daughter)\n",
    "                            matched_showers[index[0]]=pdg\n",
    "                            matched_showers_energy[index[0]] = energy\n",
    "                        if daughter in nu_track_ids:\n",
    "                            index = np.where(nu_track_ids==daughter)\n",
    "                            matched_tracks[index[0]]=pdg\n",
    "                            matched_tracks_energy[index[0]] = energy\n",
    "\n",
    "    \n",
    "    return pd.Series({\"matched_tracks\": matched_tracks, \"matched_tracks_energy\": matched_tracks_energy,\n",
    "                      \"matched_showers\": matched_showers, \"matched_showers_energy\": matched_showers_energy})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection & Features dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNB $\\nu$ + cosmics \n",
    "\n",
    "\n",
    "selection: We want to keep only the passed events but no further cuts on truth information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection uses and ordered dict\n",
    "Nu_select_true = OrderedDict([])\n",
    "Nu_select_reco = OrderedDict([(\"Passed analyzer selection\", lambda x: x[\"passed\"]==1),\n",
    "                              (\"Reco vtx in fidVol\", lambda row: CheckBorderTPC(*row[[\"vx\",\"vy\",\"vz\"]],array=fid_arr))\n",
    "                             ])\n",
    "\n",
    "# Features uses a list of functions\n",
    "Nu_feature_list =[DaughterInfo, ContainedRatio, ShowerChargeProfile, CalcRecoE, CalcDedx, OpticalInfo,MaxAngle,\n",
    "                 CC_daughter_E, ShowerTrueEnd, TrueDaughterInfo, MatchedCleanup] # The last row used true info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNB $\\nu_e$ intrinsic + cosmics \n",
    "\n",
    "\n",
    "selection: We want to do an efficiency plot, therefore we will need to have the signal definition selection. Non passed events need also to be filtered on that requirement before they are saved. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection uses and ordered dict\n",
    "Nue_select_true = OrderedDict([(\"Signal Definition 1e0p\",true_thresholds_1e0p)])\n",
    "Nue_select_reco = OrderedDict([(\"Passed analyzer selection\", lambda x: x[\"passed\"]==1)\n",
    "                               #No fiducial reco cut since we need efficiency, so keep all events\n",
    "                              ])\n",
    "\n",
    "# Features uses a list of functions\n",
    "Nue_feature_list =[DaughterInfo, ContainedRatio, ShowerChargeProfile, CalcRecoE, CalcDedx, OpticalInfo,MaxAngle,\n",
    "                   CC_daughter_E, ShowerTrueEnd, TrueDaughterInfo, MatchedCleanup] # The last row used true info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-time cosmics \n",
    "\n",
    "\n",
    "selection: Just passed events, no truth features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection uses and ordered dict\n",
    "Intime_select_true = OrderedDict([])\n",
    "Intime_select_reco = OrderedDict([(\"Passed analyzer selection\", lambda x: x[\"passed\"]==1),\n",
    "                              (\"Reco vtx in fidVol\", lambda row: CheckBorderTPC(*row[[\"vx\",\"vy\",\"vz\"]],array=fid_arr))\n",
    "                             ])\n",
    "\n",
    "# Features uses a list of functions\n",
    "Intime_feature_list =[DaughterInfo, ContainedRatio, ShowerChargeProfile, CalcRecoE, CalcDedx, OpticalInfo, MatchedCleanup,MaxAngle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BNB ext or BNB data samples\n",
    "\n",
    "\n",
    "selection: we only want to keep passed events \n",
    "\n",
    "\n",
    "We need to be carefull not adding features that depend on truth information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection uses and ordered dict\n",
    "Data_select_true = OrderedDict([])\n",
    "Data_select_reco = OrderedDict([(\"Passed analyzer selection\", lambda x: x[\"passed\"]==1),\n",
    "                                (\"Reco vtx in fidVol\", lambda row: CheckBorderTPC(*row[[\"vx\",\"vy\",\"vz\"]],array=fid_arr))\n",
    "                               ])\n",
    "                                                                                              \n",
    "\n",
    "# Features uses a list of functions\n",
    "Data_feature_list =[DaughterInfo,ContainedRatio,ShowerChargeProfile,CalcRecoE,CalcDedx,OpticalInfo,MaxAngle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function: loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(filelist,                  # List of input files\n",
    "             selection_true,            # Function dict that contains the true based selection, applied before saving the compact complete dataframw\n",
    "             selection_reco,            # Ordered dictionary with bool function that act on rows\n",
    "             featurelist,               # list of functions returning columns\n",
    "             maxf=1,                    # Maximum number of files to loop over\n",
    "             outputname=\"output\"        # Name of the final picle file\n",
    "            ):\n",
    "    \n",
    "    chunks = []                                 # list of small dataframes with all info\n",
    "    chunks_nonpassed = []                       # list of small dataframes for failed event bookkeeping\n",
    "    entries = 0                                 # entries before selection\n",
    "    counter = np.zeros(len(selection_reco)+len(selection_true))      # counts number of events passing each stage\n",
    "    total_pot = 0  # total POT of the sample\n",
    "    chuncks_pot=0\n",
    "    \n",
    "    nfiles=len(filelist)\n",
    "    if maxf<nfiles:\n",
    "        nfiles=maxf\n",
    "        \n",
    "    print (\"Start to load entries from\",nfiles,\"files.\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    progress=0\n",
    "    for i_f,fname in enumerate(filelist[:nfiles]):\n",
    "        \n",
    "        #print(fname)\n",
    "        # Store the POT of the sample\n",
    "        try:\n",
    "            df_pot = read_root(fname,\"wouterNueCC/pot\")\n",
    "            temp_pot = df_pot['pot'].sum()\n",
    "            chuncks_pot+=temp_pot\n",
    "            total_pot+=temp_pot\n",
    "            # Write this dataframe to a txtfile.\n",
    "            df_pot[['run', 'subrun']].to_csv(r'/home/wouter/Documents/Jupyter/LEE_analyzer/Input/run_subrun.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "            dftemp=read_root(fname,\"wouterNueCC/pandoratree\")\n",
    "            #print(len(dftemp.index),'entries in',fname,'.')\n",
    "            entries+=len(dftemp.index)\n",
    "        except:\n",
    "            print('Tree corrupt?', fname)\n",
    "            continue\n",
    "        \n",
    "        # Truth based selection\n",
    "        for index,(key, value) in enumerate(selection_true.items()):\n",
    "            dftemp = dftemp[dftemp.apply(value,axis=1)]\n",
    "            counter[index]+=len(dftemp.index)\n",
    "            \n",
    "        # Store some basic things about events that did not pass the selection! (but passed the truth selection)\n",
    "        #dftemp_nonpassed = dftemp[dftemp[\"passed\"]==0][[\"nu_pdg\",\"nu_E\",\"true_vx_sce\",\"true_vy_sce\",\"true_vz_sce\",\n",
    "        #                                                \"category\",\"bnbweight\"]]\n",
    "        # This compact dataframe should also have some optical information.\n",
    "        #dftemp_nonpassed = pd.concat([dftemp_nonpassed, dftemp[dftemp[\"passed\"]==0].apply(OpticalInfo,axis=1)], axis=1)\n",
    "        #dftemp_nonpassed = pd.concat([dftemp_nonpassed, dftemp[dftemp[\"passed\"]==0].apply(FlashPrecut,axis=1)], axis=1)\n",
    "        #chunks_nonpassed.append(dftemp_nonpassed)\n",
    "        \n",
    "        # Reco based selection\n",
    "        for index,(key, value) in enumerate(selection_reco.items()):\n",
    "            dftemp = dftemp[dftemp.apply(value,axis=1)]\n",
    "            counter[index+len(selection_true)]+=len(dftemp.index)\n",
    "            \n",
    "        #if no events passed, stop here\n",
    "        if(len(dftemp.index)==0):\n",
    "            continue\n",
    "        \n",
    "        # introduce the new flattened dataframe:\n",
    "        df_new = dftemp[flat_columns]\n",
    "        \n",
    "        # Reduce the dataframe size of the vector columns\n",
    "        for col in vec_columns:\n",
    "            df_new[col]=dftemp[col].apply(safely_reduce_dtype)\n",
    "        \n",
    "        # add new features to it\n",
    "        for value in featurelist:\n",
    "            df_new = pd.concat([df_new, dftemp.apply(value,axis=1)], axis=1)\n",
    "            \n",
    "        chunks.append(df_new)\n",
    "        \n",
    "        if((i_f+1) % math.ceil(nfiles/10)==0 ):\n",
    "            print (\"Progress:\",(progress+1)*10,\"%.\")\n",
    "            split_output=False\n",
    "            if(split_output):\n",
    "                print(\"Concatenating output dataframes\")\n",
    "                #Reduce the dataframe size or the non vector columns\n",
    "                print (\"POT in this chunk:\",str(chuncks_pot),\"POT.\")\n",
    "                df = pd.concat(chunks,ignore_index=True,copy=False) \n",
    "                df,_ = reduce_mem_usage(df)\n",
    "                df.to_pickle(\"../Input/\"+outputname+\"_\"+str(progress)+\".pckl\")\n",
    "                chunks=[]\n",
    "                chuncks_pot=0\n",
    "            progress+=1\n",
    "        \n",
    "       \n",
    "    end_time = time.time()\n",
    "    \n",
    "    #Reduce the dataframe size or the non vector columns\n",
    "    if len(chunks)>0:\n",
    "        print(\"Concatenating last frame in case of failure, check for double\")\n",
    "        print (\"POT in this chunk:\",str(chuncks_pot),\"POT.\")\n",
    "        df = pd.concat(chunks,ignore_index=True,copy=False) \n",
    "        df,_ = reduce_mem_usage(df)\n",
    "        df.to_pickle(\"../Input/\"+outputname+\"_\"+str(progress)+\".pckl\")\n",
    "            \n",
    "    print(\"\\nSummary:\")\n",
    "    print (entries,\"entries were loaded from\",nfiles,\"files, corresponding to\",str(total_pot),\"POT.\")\n",
    "    for key,counts in zip(list(selection_true.keys())+list(selection_reco.keys()),counter):\n",
    "        print(counts,\"Passed \",key,\" stage of selection.\")\n",
    "        \n",
    "    print(\"\\nLoading took \",sciNot(end_time-start_time),\" seconds.\")       \n",
    "    \n",
    "    #df_nonpassed = pd.concat(chunks_nonpassed,ignore_index=True,copy=False) \n",
    "    #df_nonpassed,_ = reduce_mem_usage(df_nonpassed)\n",
    "    #df_nonpassed.to_pickle(\"../Input/\"+outputname+\"_nonpassed.pckl\")\n",
    "    \n",
    "    print (\"Final dataframe has\",len(df.index),\"entries.\")\n",
    "    \n",
    "    end2_time = time.time()\n",
    "    print(\"Pickling took \",sciNot(end2_time-end_time),\" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframe and save to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load entries from 2124 files.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:238: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:42: RuntimeWarning: invalid value encountered in int_scalars\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:34: RuntimeWarning: invalid value encountered in int_scalars\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:276: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:224: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:238: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:262: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/envs/rootenv/lib/python3.4/site-packages/ipykernel/__main__.py:238: RuntimeWarning: divide by zero encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20 %.\n",
      "Progress: 30 %.\n",
      "Progress: 40 %.\n",
      "Progress: 50 %.\n",
      "Progress: 60 %.\n",
      "Progress: 70 %.\n",
      "Progress: 80 %.\n",
      "Progress: 90 %.\n",
      "Concatenating last frame in case of failure, check for double\n",
      "POT in this chunk: 0.0 POT.\n",
      "\n",
      "Summary:\n",
      "956046 entries were loaded from 2124 files, corresponding to 0.0 POT.\n",
      "41617.0 Passed  Passed analyzer selection  stage of selection.\n",
      "18122.0 Passed  Reco vtx in fidVol  stage of selection.\n",
      "\n",
      "Loading took  6748.6  seconds.\n",
      "Final dataframe has 18122 entries.\n",
      "Pickling took  12.6  seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loadData(filelist,          # List of input files\n",
    "             Intime_select_true,           # Function dict that contains the true based selection, applied before saving the compact complete dataframw\n",
    "             Intime_select_reco,           # Ordered dictionary with bool function that act on rows\n",
    "             Intime_feature_list,          # list of functions returning columns\n",
    "             maxf=3200,                  # Maximum number of files to loop over\n",
    "             outputname=\"lee\"           # Name of the final picle file\n",
    "            )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:rootenv]",
   "language": "python",
   "name": "conda-env-rootenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
