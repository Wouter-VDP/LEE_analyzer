{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEE Analyzer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes in the output directory of the jobs and convert it into a more flat pandas dataframe. \n",
    "Data from the root files will be partially processed to fields that are convenient to plot.\n",
    "The resulting dataframe will be pickled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from helpfunction import sciNot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "min_root_size = 20000  # Skip root files smaller than x bytes\n",
    "\n",
    "vtx_activity_cut = 5  # how many objects start withing 5 cm of the vertex?\n",
    "z_dead_start = 675\n",
    "z_dead_end = z_dead_start + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flat columns we want to copy from the original dataframe:\n",
    "\n",
    "flat_columns_truth = [\n",
    "    'nu_pdg',\n",
    "    'nu_E',\n",
    "    'true_vx_sce',\n",
    "    'true_vy_sce',\n",
    "    'true_vz_sce',\n",
    "    'distance',\n",
    "    'ccnc',\n",
    "    'qsqr',\n",
    "    'theta',\n",
    "    'true_1eX_signal',\n",
    "    'true_nu_fiducial',\n",
    "    'lepton_E',\n",
    "    'lepton_theta',\n",
    "    'true_vx',\n",
    "    'true_vy',\n",
    "    'true_vz',\n",
    "    ]\n",
    "\n",
    "flat_columns_reco = [\n",
    "    'event',\n",
    "    'subrun',\n",
    "    'run',\n",
    "    'category',\n",
    "    'vx',\n",
    "    'vy',\n",
    "    'vz',\n",
    "    'bnbweight',\n",
    "    'passed',\n",
    "    'candidate_pdg',\n",
    "    'numu_cuts',\n",
    "    'track_bdt_precut',\n",
    "    'n_showers',\n",
    "    'n_tracks',\n",
    "    'flash_time_max',\n",
    "    'flash_PE_max',\n",
    "    'chargecenter_x',\n",
    "    'chargecenter_y',\n",
    "    'chargecenter_z',\n",
    "    'total_spacepoint_containment',\n",
    "    'fiducial',\n",
    "    ]\n",
    "\n",
    "vec_columns_shower = [\n",
    "    'shower_open_angle',\n",
    "    'shower_length',\n",
    "    'shower_start_x',\n",
    "    'shower_start_y',\n",
    "    'shower_start_z',\n",
    "    'shower_dir_x',\n",
    "    'shower_dir_y',\n",
    "    'shower_dir_z',\n",
    "    'shower_pca',\n",
    "    'shower_maxangle',\n",
    "    'shower_vtxdistance',\n",
    "    'shower_daughter',\n",
    "    'shower_is_daughter',\n",
    "    'shower_fidvol_ratio',\n",
    "    'shower_spacepoint_dqdx_ratio',\n",
    "    'shower_dedx_hits_w',\n",
    "    'shower_dedx_w',\n",
    "    'shower_dedx_best_w',\n",
    "    'shower_energy_w',\n",
    "    'shower_hitsratio_w',\n",
    "    'shower_hits_w',\n",
    "    'shower_theta',\n",
    "    'shower_phi',\n",
    "    'shower_energy_product',\n",
    "    ]\n",
    "\n",
    "vec_columns_track = [  \n",
    "    'track_start_x',\n",
    "    'track_start_y',\n",
    "    'track_start_z',\n",
    "    'track_end_x',\n",
    "    'track_end_y',\n",
    "    'track_end_z',\n",
    "    'track_dir_x',\n",
    "    'track_dir_y',\n",
    "    'track_dir_z',\n",
    "    'track_pca',\n",
    "    'predict_em',\n",
    "    'predict_mu',\n",
    "    'predict_cos',\n",
    "    'predict_pi',\n",
    "    'predict_p',\n",
    "    'track_res_mean',\n",
    "    'track_res_std',\n",
    "    'track_maxangle',\n",
    "    'track_vtxdistance',\n",
    "    'track_daughter',\n",
    "    'track_is_daughter',\n",
    "    'track_spacepoint_dqdx_ratio',\n",
    "    'track_containment',\n",
    "    'track_dedx_hits_w',\n",
    "    'track_dedx_w',\n",
    "    'track_dedx_best_w',\n",
    "    'track_energy_w',\n",
    "    'track_hitsratio_w',\n",
    "    'track_hits_w',\n",
    "    'track_theta',\n",
    "    'track_len',\n",
    "    'track_phi',\n",
    "    ]\n",
    "\n",
    "vec_columns_truth = [\n",
    "    'true_shower_pdg',\n",
    "    'true_shower_x_sce',\n",
    "    'true_shower_y_sce',\n",
    "    'true_shower_z_sce',\n",
    "    'true_shower_depE',\n",
    "    'shower_cle',\n",
    "    'matched_showers',\n",
    "    'matched_showers_energy',\n",
    "    'track_cle',\n",
    "    'matched_tracks',\n",
    "    'matched_tracks_energy',\n",
    "    'nu_daughters_pdg',\n",
    "    'nu_daughters_E',\n",
    "    ]\n",
    "\n",
    "# Columns to use for main frame\n",
    "\n",
    "columns_data = flat_columns_reco + vec_columns_shower + vec_columns_track\n",
    "columns_mc = columns_data + flat_columns_truth + vec_columns_truth\n",
    "\n",
    "# Columns to use for track/shower frame\n",
    "\n",
    "columns_shower_mc = vec_columns_shower + ['shower_cle', 'matched_showers', 'matched_showers_energy']\n",
    "columns_track_mc = vec_columns_track + ['track_cle', 'matched_tracks', 'matched_tracks_energy']\n",
    "\n",
    "columns_flat = [\n",
    "    'bnbweight',\n",
    "    'noexpand:1<(n_showers+n_tracks)',\n",
    "    'fiducial',\n",
    "    'track_bdt_precut',\n",
    "    'n_showers',\n",
    "    'n_tracks',\n",
    "    'event',\n",
    "    'subrun',\n",
    "    'run',\n",
    "    'candidate_pdg',\n",
    "    'numu_cuts',\n",
    "    'category',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 valid ROOT files collected.\n"
     ]
    }
   ],
   "source": [
    "filelist = []\n",
    "filelist += glob.glob('/home/wouter/Templates/May/cosmic_intime_0s0t_dev/*.root')\n",
    "#filelist += glob.glob('/home/wouter/Templates/May/data_bnb_b_0s0t/*.root')\n",
    "\n",
    "print(len(filelist), 'valid ROOT files collected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function: loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list,   Input files.\n",
    "# bool,   Apply the true signal and save nonpassed events.\n",
    "# bool,   If this is data, save less stuff\n",
    "# bool,   Split output in 10 dataframes.\n",
    "# int,    Maximum number of files to loop over.\n",
    "# string, Name of the final picle file.\n",
    "def loadData(  \n",
    "    filelist,\n",
    "    signal_sample,\n",
    "    data,\n",
    "    split_output,\n",
    "    maxf=1,\n",
    "    outputname='output',\n",
    "    ):\n",
    "\n",
    "    # Create output directory:\n",
    "    if not os.path.isdir('../Input/' + outputname):\n",
    "        print (\"Output directory did not exist, creating it.\")\n",
    "        os.system(\"mkdir \"+'../Input/' + outputname)\n",
    "    else:\n",
    "        print (\"Output directory existed already\")\n",
    "                         \n",
    "    chunks = []  # list of small dataframes with all info\n",
    "    chunks_tr = []\n",
    "    chunks_sh = []\n",
    "    chunks_nonpassed = []  # list of small dataframes for failed event bookkeeping\n",
    "\n",
    "    entries = 0  # entries before selection\n",
    "    entries_signal = 0\n",
    "    flash_passed = 0\n",
    "    passed = 0\n",
    "    bdt_precut_passed = 0\n",
    "    fidvol = 0\n",
    "    cat2 = 0\n",
    "    non_passed = 0\n",
    "\n",
    "    total_pot = 0  # total POT of the sample\n",
    "    chuncks_pot = 0\n",
    "\n",
    "    columns_load = columns_data\n",
    "    columns_track = vec_columns_track\n",
    "    columns_shower = vec_columns_shower\n",
    "    if not data:\n",
    "        global columns_flat\n",
    "        columns_track = columns_track_mc\n",
    "        columns_shower = columns_shower_mc\n",
    "        columns_load = columns_mc\n",
    "        columns_flat += ['true_1eX_signal', 'lepton_theta', 'lepton_E', 'true_vz']\n",
    "\n",
    "    nfiles = len(filelist)\n",
    "    if maxf < nfiles:\n",
    "        nfiles = maxf\n",
    "\n",
    "    print('Start to load entries from', nfiles, 'files.\\n')\n",
    "    start_time = time.time()\n",
    "\n",
    "    progress = 0\n",
    "    progress_pickle = 0\n",
    "    \n",
    "    for (i_f, fname) in enumerate(filelist[:nfiles]):\n",
    "        try:\n",
    "\n",
    "            # Store the POT of the sample\n",
    "\n",
    "            df_pot = read_root(fname, 'wouterNueCC/pot')\n",
    "            temp_pot = df_pot['pot'].sum()\n",
    "            chuncks_pot += temp_pot\n",
    "            total_pot += temp_pot\n",
    "\n",
    "            # Write this dataframe to a txtfile.\n",
    "\n",
    "            df_pot[['run', 'subrun']].to_csv('../Input/' + outputname\n",
    "                    + '/run_subrun.txt', header=None, index=None,\n",
    "                    sep=' ', mode='a')\n",
    "            dftemp = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                               columns=columns_load)\n",
    "            entries += len(dftemp.index)\n",
    "\n",
    "            # Track/Shower frames\n",
    "\n",
    "            df_tr = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                              columns=columns_track + columns_flat,\n",
    "                              flatten=columns_track)\n",
    "                              \n",
    "            df_sh = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                              columns=columns_shower + columns_flat,\n",
    "                              flatten=columns_shower)\n",
    "        except (BaseException, e):\n",
    "\n",
    "            print('Tree corrupt?', fname, '\\n', str(e))\n",
    "            continue\n",
    "\n",
    "        str_eval_unresponsive_z = 'unresponsive_z = ~( @z_dead_start < true_vz < @z_dead_end)'\n",
    "\n",
    "        if signal_sample:\n",
    "            dftemp = dftemp.query('true_1eX_signal==1')\n",
    "\n",
    "            dftemp.eval(str_eval_unresponsive_z, inplace=True)\n",
    "            dftemp = dftemp.query('unresponsive_z==1')\n",
    "\n",
    "            entries_signal += dftemp['bnbweight'].sum()\n",
    "\n",
    "            # Store some basic things about events that did not pass the selection! \n",
    "            # (but passed the truth selection)\n",
    "\n",
    "            str_query = 'n_showers<1 or candidate_pdg!=12 or fiducial==0 or unresponsive_z==0'\n",
    "\n",
    "            dftemp_nonpassed = dftemp.query(str_query, inplace=False)[flat_columns_reco + flat_columns_truth]\n",
    "\n",
    "            chunks_nonpassed.append(dftemp_nonpassed)\n",
    "            non_passed += dftemp_nonpassed['bnbweight'].sum()\n",
    "\n",
    "            df_tr.eval(str_eval_unresponsive_z, inplace=True)\n",
    "            df_sh.eval(str_eval_unresponsive_z, inplace=True)\n",
    "\n",
    "            str_query = 'true_1eX_signal==1 and unresponsive_z==1'\n",
    "            df_tr.query(str_query, inplace=True)\n",
    "            df_sh.query(str_query, inplace=True)\n",
    "\n",
    "        if data or outputname == 'intime':\n",
    "            dftemp['bnbweight'] = 1\n",
    "\n",
    "        flash_passed += dftemp.query('flash_time_max>0')['bnbweight'\n",
    "                ].sum()\n",
    "\n",
    "        dftemp.query('n_showers>0 & passed==1', inplace=True)\n",
    "        passed += dftemp['bnbweight'].sum()\n",
    "\n",
    "        dftemp.query('candidate_pdg==12', inplace=True)\n",
    "        bdt_precut_passed += dftemp['bnbweight'].sum()\n",
    "\n",
    "        dftemp.query('fiducial==1', inplace=True)\n",
    "        fidvol += dftemp['bnbweight'].sum()\n",
    "\n",
    "        cat2 += dftemp.query('category==2')['bnbweight'].sum()\n",
    "\n",
    "        str_query = 'candidate_pdg==12 & fiducial==1 & n_showers>0'\n",
    "        df_tr.query(str_query, inplace=True)\n",
    "        df_tr.rename(columns={'1<(n_showers+n_tracks)': 'vtx_activity'}, inplace=True)\n",
    "        df_sh.query(str_query, inplace=True)\n",
    "        df_sh.rename(columns={'1<(n_showers+n_tracks)': 'vtx_activity'}, inplace=True)\n",
    "\n",
    "        # if no events passed, stop here\n",
    "\n",
    "        if len(dftemp.index) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add a feature:\n",
    "\n",
    "        dftemp['vtx_activity_nr'] = dftemp.apply(lambda x: \\\n",
    "                sum(x['shower_vtxdistance'] < vtx_activity_cut) \\\n",
    "                + sum(x['track_vtxdistance'] < vtx_activity_cut),\n",
    "                axis=1)\n",
    "\n",
    "        chunks.append(dftemp)\n",
    "        chunks_tr.append(df_tr)\n",
    "        chunks_sh.append(df_sh)\n",
    "\n",
    "        if (i_f + 1) % math.ceil(nfiles / 10) == 0:\n",
    "            print('Progress:', (progress + 1) * 10, '%.')\n",
    "            progress += 1\n",
    "        \n",
    "        if split_output:  \n",
    "            if (i_f + 1) % math.ceil(nfiles / split_output ) == 0:\n",
    "                print('Concatenating output dataframes')\n",
    "                print('POT in this chunk:', str(chuncks_pot), 'POT.')\n",
    "                df = pd.concat(chunks, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks = []\n",
    "                chuncks_pot = 0\n",
    "\n",
    "                df = pd.concat(chunks_sh, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_shower_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks_sh = []\n",
    "                df = pd.concat(chunks_tr, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_track_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks_tr = []\n",
    "                progress_pickle+=1\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if len(chunks) > 0:\n",
    "        if split_output:\n",
    "            progress_pickle==0\n",
    "            \n",
    "        print('Concatenating last frame in case of failure, check for double'\n",
    "              )\n",
    "        print('POT in this chunk:', str(chuncks_pot), 'POT.')\n",
    "        df = pd.concat(chunks, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_' + str(progress_pickle) + '.pckl')\n",
    "        chunks = []\n",
    "        df = pd.concat(chunks_sh, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_shower_' + str(progress_pickle) + '.pckl')\n",
    "        chunks_sh = []\n",
    "        df = pd.concat(chunks_tr, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_track_' + str(progress_pickle) + '.pckl')\n",
    "        chunks_tr = []\n",
    "\n",
    "    print('\\nSummary:')\n",
    "    print(\n",
    "        entries,\n",
    "        'entries were loaded from',\n",
    "        nfiles,\n",
    "        'files, corresponding to',\n",
    "        str(total_pot),\n",
    "        'POT.',\n",
    "        )\n",
    "    if signal_sample:\n",
    "        print(int(entries_signal), 'events are 1eX signal in fidvol.')\n",
    "        if non_passed + fidvol != entries_signal:\n",
    "            print(\n",
    "                'ERROR: the passing (',\n",
    "                fidvol,\n",
    "                ') and non-passing (',\n",
    "                non_passed,\n",
    "                ') events did not sum up correctly to',\n",
    "                entries_signal,\n",
    "                '!',\n",
    "                )\n",
    "\n",
    "    print(int(flash_passed), ' events pass the optical precuts.')\n",
    "    print(int(passed), 'events pass the selection (if topo).')\n",
    "    print(int(bdt_precut_passed), 'events pass the track bdt precut.')\n",
    "    print(int(fidvol), 'events are in the fiducial volume.')\n",
    "    print(int(cat2), 'events are category electron neutrino.')\n",
    "\n",
    "    print('\\nLoading took ', sciNot(end_time - start_time), ' seconds.')\n",
    "\n",
    "    if signal_sample:\n",
    "        df_nonpassed = pd.concat(chunks_nonpassed, ignore_index=True, copy=False)\n",
    "        df_nonpassed.to_pickle('../Input/' + outputname + '/' + outputname + '_nonpassed.pckl')\n",
    "\n",
    "    end2_time = time.time()\n",
    "    print('Pickling took ', sciNot(end2_time - end_time), ' seconds.')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframe and save to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory existed already\n",
      "Start to load entries from 255 files.\n",
      "\n",
      "Progress: 10 %.\n",
      "Progress: 20 %.\n",
      "Progress: 30 %.\n",
      "Progress: 40 %.\n",
      "Progress: 50 %.\n",
      "Progress: 60 %.\n",
      "Concatenating last frame in case of failure, check for double\n",
      "POT in this chunk: 0.0 POT.\n",
      "\n",
      "Summary:\n",
      "115323 entries were loaded from 255 files, corresponding to 0.0 POT.\n",
      "13904  events pass the optical precuts.\n",
      "4937 events pass the selection (if topo).\n",
      "502 events pass the track bdt precut.\n",
      "296 events are in the fiducial volume.\n",
      "0 events are category electron neutrino.\n",
      "\n",
      "Loading took  50.1  seconds.\n",
      "Pickling took  1.6  seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# list,   Input files.\n",
    "# bool,   Apply the true signal and save nonpassed events.\n",
    "# bool,   If this is data, save less stuff\n",
    "# bool,   Split output in 10 dataframes.\n",
    "# int,    Maximum number of files to loop over.\n",
    "# string, Name of the final picle file.       \n",
    "loadData( \n",
    "    filelist,\n",
    "    signal_sample=False,\n",
    "    data=True,\n",
    "    split_output=0,\n",
    "    maxf=5000,\n",
    "    outputname='bnb',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e2bfcab00a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Input/lee_75k/lee_75k_1.pckl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bnbweight\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n_tracks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n_showers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wouter/Binaries/miniconda3/envs/rootenv/lib/python3.4/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# compat pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Input/lee_75k/lee_75k_1.pckl'"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../Input/lee_75k/lee_75k_1.pckl\")\n",
    "df[[\"bnbweight\",\"n_tracks\",\"n_showers\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../Input/lee/lee_shower_10.pckl\")\n",
    "df[[\"bnbweight\",\"matched_showers_energy\",\"vtx_activity\"]][15:30]\n",
    "df.hist(\"vtx_activity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../Input/lee_75k/lee_75k_track_1.pckl\")\n",
    "df[[\"track_energy_w\",\"matched_tracks_energy\",\"vtx_activity\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:rootenv]",
   "language": "python",
   "name": "conda-env-rootenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
