{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEE Analyzer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes in the output directory of the jobs and convert it into a more flat pandas dataframe. \n",
    "Data from the root files will be partially processed to fields that are convenient to plot.\n",
    "The resulting dataframe will be pickled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from helpfunction import sciNot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "min_root_size = 20000  # Skip root files smaller than x bytes\n",
    "\n",
    "vtx_activity_cut = 5  # how many objects start withing 5 cm of the vertex?\n",
    "z_dead_start = 675\n",
    "z_dead_end = z_dead_start + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flat columns we want to copy from the original dataframe:\n",
    "\n",
    "flat_columns_truth = [\n",
    "    'nu_pdg',\n",
    "    'nu_E',\n",
    "    'true_vx_sce',\n",
    "    'true_vy_sce',\n",
    "    'true_vz_sce',\n",
    "    'distance',\n",
    "    'ccnc',\n",
    "    'qsqr',\n",
    "    'theta',\n",
    "    'true_1eX_signal',\n",
    "    'true_nu_fiducial',\n",
    "    'lepton_E',\n",
    "    'lepton_theta',\n",
    "    'true_vx',\n",
    "    'true_vy',\n",
    "    'true_vz',\n",
    "    ]\n",
    "\n",
    "flat_columns_reco = [\n",
    "    'event',\n",
    "    'subrun',\n",
    "    'run',\n",
    "    'category',\n",
    "    'vx',\n",
    "    'vy',\n",
    "    'vz',\n",
    "    'bnbweight',\n",
    "    'passed',\n",
    "    'candidate_pdg',\n",
    "    'numu_cuts',\n",
    "    'track_bdt_precut',\n",
    "    'n_showers',\n",
    "    'n_tracks',\n",
    "    #'flash_time_max',\n",
    "    'flash_PE_max',\n",
    "    'chargecenter_x',\n",
    "    'chargecenter_y',\n",
    "    'chargecenter_z',\n",
    "    'total_spacepoint_containment',\n",
    "    'fiducial',\n",
    "    ]\n",
    "\n",
    "vec_columns_shower = [\n",
    "    'shower_open_angle',\n",
    "    'shower_length',\n",
    "    'shower_start_x',\n",
    "    'shower_start_y',\n",
    "    'shower_start_z',\n",
    "    'shower_dir_x',\n",
    "    'shower_dir_y',\n",
    "    'shower_dir_z',\n",
    "    'shower_pca',\n",
    "    'shower_maxangle',\n",
    "    'shower_vtxdistance',\n",
    "    'shower_daughter',\n",
    "    'shower_is_daughter',\n",
    "    'shower_fidvol_ratio',\n",
    "    'shower_spacepoint_dqdx_ratio',\n",
    "    'shower_dedx_hits_w',\n",
    "    'shower_dedx_w',\n",
    "    'shower_dedx_best_w',\n",
    "    'shower_energy_w',\n",
    "    'shower_hitsratio_w',\n",
    "    'shower_hits_w',\n",
    "    'shower_theta',\n",
    "    'shower_phi',\n",
    "    'shower_energy_product',\n",
    "    'shower_pitch2',\n",
    "    'shower_ncluster'\n",
    "    ]\n",
    "\n",
    "vec_columns_track = [  \n",
    "    'track_start_x',\n",
    "    'track_start_y',\n",
    "    'track_start_z',\n",
    "    'track_end_x',\n",
    "    'track_end_y',\n",
    "    'track_end_z',\n",
    "    'track_dir_x',\n",
    "    'track_dir_y',\n",
    "    'track_dir_z',\n",
    "    'track_pca',\n",
    "    'predict_em',\n",
    "    'predict_mu',\n",
    "    'predict_cos',\n",
    "    'predict_pi',\n",
    "    'predict_p',\n",
    "    'track_res_mean',\n",
    "    'track_res_std',\n",
    "    'track_maxangle',\n",
    "    'track_vtxdistance',\n",
    "    'track_daughter',\n",
    "    'track_is_daughter',\n",
    "    'track_spacepoint_dqdx_ratio',\n",
    "    'track_containment',\n",
    "    'track_dedx_hits_w',\n",
    "    'track_dedx_w',\n",
    "    'track_dedx_best_w',\n",
    "    'track_energy_w',\n",
    "    'track_hitsratio_w',\n",
    "    'track_hits_w',\n",
    "    'track_theta',\n",
    "    'track_len',\n",
    "    'track_phi',\n",
    "    ]\n",
    "\n",
    "vec_columns_truth = [\n",
    "    'true_shower_pdg',\n",
    "    'true_shower_x_sce',\n",
    "    'true_shower_y_sce',\n",
    "    'true_shower_z_sce',\n",
    "    'true_shower_depE',\n",
    "    'shower_cle',\n",
    "    'matched_showers',\n",
    "    'matched_showers_energy',\n",
    "    'track_cle',\n",
    "    'matched_tracks',\n",
    "    'matched_tracks_energy',\n",
    "    'nu_daughters_pdg',\n",
    "    'nu_daughters_E',\n",
    "    ]\n",
    "\n",
    "# Columns to use for main frame\n",
    "\n",
    "columns_data = flat_columns_reco + vec_columns_shower + vec_columns_track\n",
    "columns_mc = columns_data + flat_columns_truth + vec_columns_truth\n",
    "\n",
    "# Columns to use for track/shower frame\n",
    "\n",
    "columns_shower_mc = vec_columns_shower + ['shower_cle', 'matched_showers', 'matched_showers_energy']\n",
    "columns_track_mc = vec_columns_track + ['track_cle', 'matched_tracks', 'matched_tracks_energy']\n",
    "\n",
    "columns_flat = [\n",
    "    'bnbweight',\n",
    "    'noexpand:1<(n_showers+n_tracks)',\n",
    "    'fiducial',\n",
    "    'track_bdt_precut',\n",
    "    'n_showers',\n",
    "    'n_tracks',\n",
    "    'event',\n",
    "    'subrun',\n",
    "    'run',\n",
    "    'candidate_pdg',\n",
    "    'numu_cuts',\n",
    "    'category',\n",
    "    \"flash_passed\",\n",
    "    'flash_PE_max',\n",
    "    'vx',\n",
    "    'vy',\n",
    "    'vz',\n",
    "    'passed',\n",
    "    'candidate_pdg',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFileList(directories):\n",
    "    directory_in = '/home/wouter/Templates/July/'\n",
    "    filelist = []\n",
    "    for directory in directories:\n",
    "        filelist += glob.glob(directory_in+directory+\"/*.root\")\n",
    "    print(len(filelist), 'valid ROOT files collected.')\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function: loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list,   Input files.\n",
    "# bool,   Apply the true signal and save nonpassed events.\n",
    "# bool,   If this is data, save less stuff\n",
    "# bool,   Split output in 10 dataframes.\n",
    "# int,    Maximum number of files to loop over.\n",
    "# string, Name of the final picle file.\n",
    "def loadData(  \n",
    "    filelist,\n",
    "    signal_sample,\n",
    "    data,\n",
    "    split_output,\n",
    "    maxf=1,\n",
    "    outputname='output',\n",
    "    ):\n",
    "\n",
    "    # Create output directory:\n",
    "    if not os.path.isdir('../Input/' + outputname):\n",
    "        print (\"Output directory did not exist, creating it.\")\n",
    "        os.system(\"mkdir \"+'../Input/' + outputname)\n",
    "    else:\n",
    "        print (\"Output directory existed already\")\n",
    "                         \n",
    "    chunks = []  # list of small dataframes with all info\n",
    "    chunks_tr = []\n",
    "    chunks_sh = []\n",
    "    chunks_nonpassed = []  # list of small dataframes for failed event bookkeeping\n",
    "\n",
    "    # We store the number of events/wieghted after each selection step\n",
    "    entries = 0\n",
    "    entries_weight = 0\n",
    "    entries_signal = 0\n",
    "    entries_signal_weight = 0\n",
    "    flash_time_weight = 0\n",
    "    flash_50_weight = 0\n",
    "    flash_passed = 0\n",
    "    flash_passed_weight = 0\n",
    "    passed = 0\n",
    "    passed_weight = 0\n",
    "    pdg12_passed = 0\n",
    "    pdg12_passed_weight = 0\n",
    "    fidvol = 0\n",
    "    fidvol_weight = 0\n",
    "    cat2 = 0\n",
    "    non_passed = 0\n",
    "    non_passed_weight = 0\n",
    "\n",
    "    total_pot = 0\n",
    "    chuncks_pot = 0  # total POT of the sample\n",
    "\n",
    "    columns_load = columns_data\n",
    "    columns_track = vec_columns_track\n",
    "    columns_shower = vec_columns_shower\n",
    "    \n",
    "    if not data:\n",
    "        global columns_flat\n",
    "        columns_track = columns_track_mc\n",
    "        columns_shower = columns_shower_mc\n",
    "        columns_load = columns_mc\n",
    "        columns_flat += ['true_1eX_signal', 'lepton_theta', 'lepton_E', 'true_vz']\n",
    "\n",
    "    nfiles = len(filelist)\n",
    "    if maxf < nfiles:\n",
    "        nfiles = maxf\n",
    "\n",
    "    print('Start to load entries from', nfiles, 'files.\\n')\n",
    "    start_time = time.time()\n",
    "\n",
    "    progress = 0\n",
    "    progress_pickle = 0\n",
    "    \n",
    "    for (i_f, fname) in enumerate(filelist[:nfiles]):\n",
    "        try:\n",
    "\n",
    "            # Store the POT of the sample\n",
    "\n",
    "            df_pot = read_root(fname, 'wouterNueCC/pot')\n",
    "            temp_pot = df_pot['pot'].sum()\n",
    "            chuncks_pot += temp_pot\n",
    "            total_pot += temp_pot\n",
    "\n",
    "            # Write this dataframe to a txtfile.\n",
    "\n",
    "            df_pot[['run', 'subrun']].to_csv('../Input/' + outputname\n",
    "                    + '/run_subrun.txt', header=None, index=None,\n",
    "                    sep=' ', mode='a')\n",
    "            dftemp = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                               columns=columns_load)\n",
    "            \n",
    "            if data or outputname == 'intime':\n",
    "                dftemp['bnbweight'] = 1\n",
    "\n",
    "            entries += len(dftemp.index)\n",
    "            entries_weight += dftemp['bnbweight'].sum()\n",
    "            # Track/Shower frames\n",
    "\n",
    "            df_tr = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                              columns=columns_track + columns_flat,\n",
    "                              flatten=columns_track)\n",
    "                              \n",
    "            df_sh = read_root(fname, 'wouterNueCC/pandoratree',\n",
    "                              columns=columns_shower + columns_flat,\n",
    "                              flatten=columns_shower)\n",
    "        except (BaseException, e):\n",
    "\n",
    "            print('Tree corrupt?', fname, '\\n', str(e))\n",
    "            continue\n",
    "\n",
    "        str_eval_unresponsive_z = 'unresponsive_z = ~( @z_dead_start < true_vz < @z_dead_end)'\n",
    "        \n",
    "        str_eval_unresponsive_reco_z = 'unresponsive_reco_z = ~( @z_dead_start < vz < @z_dead_end)'\n",
    "        dftemp.eval(str_eval_unresponsive_reco_z, inplace=True)\n",
    "\n",
    "        if signal_sample:\n",
    "            dftemp = dftemp.query('true_1eX_signal==1')\n",
    "\n",
    "            dftemp.eval(str_eval_unresponsive_z, inplace=True)\n",
    "            dftemp = dftemp.query('unresponsive_z==1')\n",
    "            \n",
    "            entries_signal += len(dftemp.index)\n",
    "            entries_signal_weight += dftemp['bnbweight'].sum()\n",
    "\n",
    "            # Store some basic things about events that did not pass the selection! \n",
    "            # (but passed the truth selection)\n",
    "\n",
    "            str_query = 'n_showers<1 or candidate_pdg!=12 or fiducial==0 or unresponsive_reco_z==0'\n",
    "\n",
    "            dftemp_nonpassed = dftemp.query(str_query, inplace=False)[flat_columns_reco + flat_columns_truth]\n",
    "\n",
    "            chunks_nonpassed.append(dftemp_nonpassed)\n",
    "            non_passed += len(dftemp_nonpassed.index)\n",
    "            non_passed_weight += dftemp_nonpassed['bnbweight'].sum()\n",
    "\n",
    "            df_tr.eval(str_eval_unresponsive_z, inplace=True)\n",
    "            df_sh.eval(str_eval_unresponsive_z, inplace=True)\n",
    "\n",
    "            str_query = 'true_1eX_signal==1 and unresponsive_z==1'\n",
    "            df_tr.query(str_query, inplace=True)\n",
    "            df_sh.query(str_query, inplace=True)\n",
    "        \n",
    "        dftemp.query('flash_PE_max>0', inplace=True)\n",
    "        flash_time_weight += dftemp['bnbweight'].sum()\n",
    "        \n",
    "        dftemp.query('flash_PE_max>50', inplace=True)\n",
    "        flash_50_weight += dftemp['bnbweight'].sum()\n",
    "        \n",
    "        dftemp.query('passed==1', inplace=True)\n",
    "        flash_passed += len(dftemp.index)\n",
    "        flash_passed_weight += dftemp['bnbweight'].sum()\n",
    "        \n",
    "        dftemp.query('n_showers>0', inplace=True)\n",
    "        passed += len(dftemp.index)\n",
    "        passed_weight += dftemp['bnbweight'].sum()\n",
    "\n",
    "        dftemp.query('candidate_pdg==12', inplace=True)\n",
    "        pdg12_passed += len(dftemp.index)\n",
    "        pdg12_passed_weight += dftemp['bnbweight'].sum()\n",
    "\n",
    "        dftemp.query('fiducial==1 & unresponsive_reco_z==1', inplace=True)\n",
    "        fidvol += len(dftemp.index)\n",
    "        fidvol_weight += dftemp['bnbweight'].sum()\n",
    "\n",
    "        cat2 += dftemp.query('category==2')['bnbweight'].sum()\n",
    "\n",
    "        str_query = 'candidate_pdg==12 & fiducial==1 & n_showers>0 & flash_PE_max>50 & passed==1'\n",
    "        df_tr.query(str_query, inplace=True)\n",
    "        df_tr.rename(columns={'1<(n_showers+n_tracks)': 'vtx_activity'}, inplace=True)\n",
    "        df_sh.query(str_query, inplace=True)\n",
    "        df_sh.rename(columns={'1<(n_showers+n_tracks)': 'vtx_activity'}, inplace=True)\n",
    "\n",
    "        # if no events passed, stop here\n",
    "\n",
    "        if len(dftemp.index) == 0:\n",
    "            continue\n",
    "\n",
    "        # Add a feature:\n",
    "\n",
    "        dftemp['vtx_activity_nr'] = dftemp.apply(lambda x: \\\n",
    "                sum(x['shower_vtxdistance'] < vtx_activity_cut) \\\n",
    "                + sum(x['track_vtxdistance'] < vtx_activity_cut),\n",
    "                axis=1)\n",
    "\n",
    "        chunks.append(dftemp)\n",
    "        chunks_tr.append(df_tr)\n",
    "        chunks_sh.append(df_sh)\n",
    "\n",
    "        if (i_f + 1) % math.ceil(nfiles / 10) == 0:\n",
    "            print('Progress:', (progress + 1) * 10, '%.')\n",
    "            progress += 1\n",
    "        \n",
    "        if split_output:  \n",
    "            if (i_f + 1) % math.ceil(nfiles / split_output ) == 0:\n",
    "                print('Concatenating output dataframes')\n",
    "                print('POT in this chunk:', str(chuncks_pot), 'POT.')\n",
    "                df = pd.concat(chunks, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks = []\n",
    "                chuncks_pot = 0\n",
    "\n",
    "                df = pd.concat(chunks_sh, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_shower_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks_sh = []\n",
    "                df = pd.concat(chunks_tr, ignore_index=True, copy=False)\n",
    "                df.to_pickle('../Input/' + outputname + '/'\n",
    "                             + outputname + '_track_' + str(progress_pickle)\n",
    "                             + '.pckl')\n",
    "                chunks_tr = []\n",
    "                progress_pickle+=1\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if len(chunks) > 0:\n",
    "        if split_output:\n",
    "            progress_pickle==0\n",
    "            \n",
    "        print('Concatenating last frame in case of failure, check for double'\n",
    "              )\n",
    "        print('POT in this chunk:', str(chuncks_pot), 'POT.')\n",
    "        df = pd.concat(chunks, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_' + str(progress_pickle) + '.pckl')\n",
    "        chunks = []\n",
    "        df = pd.concat(chunks_sh, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_shower_' + str(progress_pickle) + '.pckl')\n",
    "        chunks_sh = []\n",
    "        df = pd.concat(chunks_tr, ignore_index=True, copy=False)\n",
    "        df.to_pickle('../Input/' + outputname + '/' + outputname + '_track_' + str(progress_pickle) + '.pckl')\n",
    "        chunks_tr = []\n",
    "\n",
    "    print('\\nSummary:')\n",
    "    print(\n",
    "        entries,\n",
    "        'entries were loaded from',\n",
    "        nfiles,\n",
    "        'files, corresponding to',\n",
    "        str(total_pot),\n",
    "        'POT.',\n",
    "        )\n",
    "    if signal_sample:\n",
    "        print(round(entries_signal), 'events are 1eX signal in fidvol.')\n",
    "        print(round(entries_signal_weight), 'weighted signal entries to start with.')\n",
    "        if round(non_passed + fidvol) != round(entries_signal):\n",
    "            print(\n",
    "                  'ERROR: the passing (',fidvol,\n",
    "                  ') and non-passing (',non_passed,\n",
    "                  ') events did not sum up correctly to',entries_signal,'!'\n",
    "                 )\n",
    "    \n",
    "    print(round(entries_weight), ' weighted entries to start with.')\n",
    "    print(round(flash_time_weight), ' events have a flash.')\n",
    "    print(round(flash_50_weight), ' events have a 50PE flash.')\n",
    "    print(round(flash_passed_weight), ' events pass the optical precuts.')\n",
    "    print(round(passed_weight), 'events have a shower.')\n",
    "    print(round(pdg12_passed_weight), 'events pass the category 12.')\n",
    "    print(round(fidvol_weight), 'events are in the fiducial volume.')\n",
    "    print(round(cat2), 'events are category electron neutrino.')\n",
    "\n",
    "    print('\\nLoading took ', sciNot(end_time - start_time), ' seconds.')\n",
    "\n",
    "    if signal_sample:\n",
    "        df_nonpassed = pd.concat(chunks_nonpassed, ignore_index=True, copy=False)\n",
    "        df_nonpassed.to_pickle('../Input/' + outputname + '/' + outputname + '_nonpassed.pckl')\n",
    "\n",
    "    end2_time = time.time()\n",
    "    print('Pickling took ', sciNot(end2_time - end_time), ' seconds.')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframe and save to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 valid ROOT files collected.\n",
      "beam_off\n",
      "Output directory did not exist, creating it.\n",
      "Start to load entries from 741 files.\n",
      "\n",
      "Progress: 10 %.\n",
      "Progress: 20 %.\n",
      "Progress: 30 %.\n",
      "Progress: 40 %.\n",
      "Progress: 50 %.\n",
      "Progress: 60 %.\n",
      "Progress: 70 %.\n",
      "Progress: 80 %.\n",
      "Progress: 90 %.\n",
      "Concatenating last frame in case of failure, check for double\n",
      "POT in this chunk: 143234574.854 POT.\n",
      "\n",
      "Summary:\n",
      "433388 entries were loaded from 741 files, corresponding to 143234574.854 POT.\n",
      "433388  weighted entries to start with.\n",
      "295955  events have a flash.\n",
      "294118  events have a 50PE flash.\n",
      "143086  events pass the optical precuts.\n",
      "59927 events have a shower.\n",
      "10896 events pass the category 12.\n",
      "5865 events are in the fiducial volume.\n",
      "0 events are category electron neutrino.\n",
      "\n",
      "Loading took  181.1  seconds.\n",
      "Pickling took  9.7  seconds.\n",
      "Done!\n",
      "\n",
      "\n",
      "\n",
      "282 valid ROOT files collected.\n",
      "nue\n",
      "Output directory did not exist, creating it.\n",
      "Start to load entries from 282 files.\n",
      "\n",
      "Progress: 10 %.\n",
      "Progress: 20 %.\n",
      "Progress: 30 %.\n",
      "Progress: 40 %.\n",
      "Progress: 50 %.\n",
      "Progress: 60 %.\n",
      "Progress: 70 %.\n",
      "Progress: 80 %.\n",
      "Progress: 90 %.\n",
      "Concatenating last frame in case of failure, check for double\n",
      "POT in this chunk: 2.41757769545e+22 POT.\n",
      "\n",
      "Summary:\n",
      "197400 entries were loaded from 282 files, corresponding to 2.41757769545e+22 POT.\n",
      "41913 events are 1eX signal in fidvol.\n",
      "30955.0 weighted signal entries to start with.\n",
      "ERROR: the passing ( 25987 ) and non-passing ( 15675 ) events did not sum up correctly to 41913 !\n",
      "147757.0  weighted entries to start with.\n",
      "30305.0  events have a flash.\n",
      "30304.0  events have a 50PE flash.\n",
      "29239.0  events pass the optical precuts.\n",
      "26421.0 events have a shower.\n",
      "21096.0 events pass the category 12.\n",
      "19660.0 events are in the fiducial volume.\n",
      "15640.0 events are category electron neutrino.\n",
      "\n",
      "Loading took  139.3  seconds.\n",
      "Pickling took  20.6  seconds.\n",
      "Done!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all optical selection frames! \n",
    "beam_on = {\n",
    "    \"filelist\": [\"data_bnb_a_0s0t\", \"data_bnb_b_0s0t\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": True,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"beam_on\"\n",
    "}\n",
    "\n",
    "beam_off = {\n",
    "    \"filelist\": [\"data_bnbext_a_0s0t\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": True,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"beam_off\"\n",
    "}\n",
    "\n",
    "nu = {\n",
    "    \"filelist\": [\"bnb_nu_cosmic_0s0t_dev\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": False,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nu\"\n",
    "}\n",
    "\n",
    "nue = {\n",
    "    \"filelist\": [\"bnb_nue_cosmic_0s0t_dev\"],\n",
    "    \"signal_sample\": True,\n",
    "    \"data\": False,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nue\"\n",
    "}\n",
    "\n",
    "nu_lightbug = {\n",
    "    \"filelist\": [\"bnb_nu_cosmic_lightbug\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": False,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nu_lightbug\"\n",
    "}\n",
    "\n",
    "nu_induced = {\n",
    "    \"filelist\": [\"bnb_nu_cosmic_inducedcharge\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": False,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nu_induced\"\n",
    "}\n",
    "\n",
    "nu_tune3 = {\n",
    "    \"filelist\": [\"bnb_nu_cosmic_tune3_0s0t_dev\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": False,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nu_tune3\"\n",
    "}\n",
    "\n",
    "nu_overlaid = {\n",
    "    \"filelist\": [\"bnb_nu_cosmic_overlaid_0s0t_dev\"],\n",
    "    \"signal_sample\": False,\n",
    "    \"data\": True,\n",
    "    \"split_output\": 0,\n",
    "    \"maxf\" : 1000,\n",
    "    \"outputname\" : \"nu_overlaid\"\n",
    "}\n",
    "\n",
    "\n",
    "# add overlaid, cv, lightbug, tune3, (induced charge)\n",
    "#samples_dict = [nue, beam_on, beam_off, nu, nu_lightbug, nu_induced ,nu_tune3, nu_overlaid]\n",
    "samples_dict = [beam_off, nue]\n",
    "\n",
    "for s in samples_dict:\n",
    "    filelist = getFileList(s[\"filelist\"])\n",
    "    print(s[\"outputname\"])\n",
    "    loadData( \n",
    "    filelist,\n",
    "    signal_sample = s[\"signal_sample\"],\n",
    "    data = s[\"data\"],\n",
    "    split_output = s[\"split_output\"],\n",
    "    maxf = s[\"maxf\"],\n",
    "    outputname = s[\"outputname\"],\n",
    "    )\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory did not exist, creating it.\n",
      "Start to load entries from 282 files.\n",
      "\n",
      "Progress: 10 %.\n",
      "Progress: 20 %.\n",
      "Progress: 30 %.\n",
      "Progress: 40 %.\n",
      "Progress: 50 %.\n",
      "Progress: 60 %.\n",
      "Progress: 70 %.\n",
      "Progress: 80 %.\n",
      "Progress: 90 %.\n",
      "Concatenating last frame in case of failure, check for double\n",
      "POT in this chunk: 2.41757769545e+22 POT.\n",
      "\n",
      "Summary:\n",
      "197400 entries were loaded from 282 files, corresponding to 2.41757769545e+22 POT.\n",
      "197400  weighted entries to start with.\n",
      "161695  events have a flash.\n",
      "158611  events have a 50PE flash.\n",
      "111006  events pass the optical precuts.\n",
      "77131 events have a shower.\n",
      "51656 events pass the category 12.\n",
      "33944 events are in the fiducial volume.\n",
      "23218 events are category electron neutrino.\n",
      "\n",
      "Loading took  89.2  seconds.\n",
      "Pickling took  23.5  seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# list,   Input files.\n",
    "# bool,   Apply the true signal and save nonpassed events.\n",
    "# bool,   If this is data, save less stuff\n",
    "# bool,   Split output in 10 dataframes.\n",
    "# int,    Maximum number of files to loop over.\n",
    "# string, Name of the final picle file.       \n",
    "loadData( \n",
    "    filelist,\n",
    "    signal_sample=False,\n",
    "    data=True,\n",
    "    split_output=0,\n",
    "    maxf=5000,\n",
    "    outputname='bnb',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:rootenv]",
   "language": "python",
   "name": "conda-env-rootenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
